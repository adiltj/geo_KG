2024-06-18 16:35:34 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 16:35:34 - INFO - [inference.py:222] -  ### 将当前配置打印到日志文件中 
2024-06-18 16:35:35 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 16:35:35 - INFO - [inference.py:234] - ## 成功载入已有模型，进行预测......
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:35 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:44 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:44 - INFO - [inference.py:303] - --------Dataset Build!--------
2024-06-18 16:35:44 - INFO - [inference.py:307] - --------Get Data-loader!--------
2024-06-18 16:35:44 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\experiments\clue\config.json
2024-06-18 16:35:44 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 16:35:44 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\experiments\clue\pytorch_model.bin
2024-06-18 16:35:46 - INFO - [inference.py:312] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\experiments\clue--------
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:35:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 16:42:51 - INFO - [inference.py:218] -  ### 将当前配置打印到日志文件中 
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  device = cpu
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  split_sep = _!_
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  is_sample_shuffle = True
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  batch_size = 999
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  max_sen_len = None
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  num_labels = 15
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  epochs = 5
2024-06-18 16:42:51 - INFO - [inference.py:220] - ###  model_val_per_epoch = 2
2024-06-18 16:43:15 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 16:43:15 - INFO - [inference.py:218] -  ### 将当前配置打印到日志文件中 
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  device = cpu
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  split_sep = _!_
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  is_sample_shuffle = True
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  batch_size = 999
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  max_sen_len = None
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  num_labels = 15
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  epochs = 5
2024-06-18 16:43:15 - INFO - [inference.py:220] - ###  model_val_per_epoch = 2
2024-06-18 17:02:42 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:02:42 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  device = cuda:0
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  train_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\train_0821.txt
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  val_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\val_0821.txt
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  test_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\test_0821.txt
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\cache
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  split_sep = _!_
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  is_sample_shuffle = True
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  batch_size = 16
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  num_labels = 10
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  epochs = 1
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  model_val_per_epoch = 1
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:02:42 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:02:43 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:02:43 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:16 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:03:16 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  device = cpu
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  train_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\train_0821.txt
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  val_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\val_0821.txt
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  test_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\test_0821.txt
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\cache
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  split_sep = _!_
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  is_sample_shuffle = True
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  batch_size = 16
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  num_labels = 10
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  epochs = 1
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  model_val_per_epoch = 1
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:03:16 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:03:17 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:22 - INFO - [inference.py:331] - --------Dataset Build!--------
2024-06-18 17:03:22 - INFO - [inference.py:335] - --------Get Data-loader!--------
2024-06-18 17:03:22 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 17:03:22 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 17:03:22 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 17:03:24 - INFO - [inference.py:340] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:03:24 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:15 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:04:15 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  device = cpu
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  train_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\train_0821.txt
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  val_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\val_0821.txt
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  test_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\test_0821.txt
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\cache
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  split_sep = _!_
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  is_sample_shuffle = True
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  batch_size = 16
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  num_labels = 10
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  epochs = 1
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  model_val_per_epoch = 1
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:04:15 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:04:16 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:22 - INFO - [inference.py:331] - --------Dataset Build!--------
2024-06-18 17:04:22 - INFO - [inference.py:335] - --------Get Data-loader!--------
2024-06-18 17:04:22 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 17:04:22 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 17:04:22 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 17:04:23 - INFO - [inference.py:340] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:04:23 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:25 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:05:25 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  device = cpu
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  train_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\train_0821.txt
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  val_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\val_0821.txt
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  test_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\test_0821.txt
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\cache
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  split_sep = _!_
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  is_sample_shuffle = True
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  batch_size = 999
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  num_labels = 10
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  epochs = 5
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  model_val_per_epoch = 2
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:05:25 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:05:26 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:36 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:36 - INFO - [inference.py:331] - --------Dataset Build!--------
2024-06-18 17:05:36 - INFO - [inference.py:335] - --------Get Data-loader!--------
2024-06-18 17:05:36 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 17:05:36 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 17:05:36 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 17:05:37 - INFO - [inference.py:340] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:05:37 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:07:26 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:07:26 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  device = cpu
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  train_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\train_0821.txt
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  val_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\val_0821.txt
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  test_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\test_0821.txt
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  split_sep = _!_
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  is_sample_shuffle = True
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  batch_size = 999
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  num_labels = 10
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  epochs = 5
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  model_val_per_epoch = 2
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:07:26 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:07:27 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:08:11 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:08:11 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  device = cpu
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  train_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\train_0821.txt
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  val_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\val_0821.txt
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  test_file_path = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification\test_0821.txt
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  logs_save_dir = e:/pythonProject/pytorch/ExtractInfo\logs
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  split_sep = _!_
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  is_sample_shuffle = True
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  batch_size = 999
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  num_labels = 15
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  epochs = 5
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  model_val_per_epoch = 2
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:08:11 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:08:12 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:08:12 - INFO - [inference.py:262] - ## 成功载入已有模型，进行预测......
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:12 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:21 - INFO - [inference.py:331] - --------Dataset Build!--------
2024-06-18 17:08:21 - INFO - [inference.py:335] - --------Get Data-loader!--------
2024-06-18 17:08:21 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 17:08:21 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 17:08:21 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 17:08:22 - INFO - [inference.py:340] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:08:22 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:09:16 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:09:16 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  device = cpu
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  batch_size = 999
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  num_labels = 15
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  epochs = 5
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  model_val_per_epoch = 2
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:09:16 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:09:17 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:09:18 - INFO - [inference.py:262] - ## 成功载入已有模型，进行预测......
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:09:18 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:15 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:12:15 - INFO - [inference.py:251] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  device = cpu
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  batch_size = 999
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  max_sen_len = None
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  num_labels = 15
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  epochs = 5
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  model_val_per_epoch = 2
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  vocab_size = 21128
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  hidden_size = 768
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  num_hidden_layers = 12
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  num_attention_heads = 12
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  hidden_act = gelu
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  intermediate_size = 3072
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  pad_token_id = 0
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  max_position_embeddings = 512
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  type_vocab_size = 2
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  initializer_range = 0.02
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  directionality = bidi
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  pooler_fc_size = 768
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  pooler_num_attention_heads = 12
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  pooler_num_fc_layers = 3
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  pooler_size_per_head = 128
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  pooler_type = first_token_transform
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  lstm_embedding_size = 768
2024-06-18 17:12:15 - INFO - [inference.py:253] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:12:16 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:12:16 - INFO - [inference.py:262] - ## 成功载入已有模型，进行预测......
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:16 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:25 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:25 - INFO - [inference.py:326] - --------Dataset Build!--------
2024-06-18 17:12:25 - INFO - [inference.py:330] - --------Get Data-loader!--------
2024-06-18 17:12:25 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 17:12:25 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 17:12:25 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 17:12:26 - INFO - [inference.py:335] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:12:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:14:49 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:15:30 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:16:15 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 17:16:15 - INFO - [inference.py:241] -  ### 将当前配置打印到日志文件中 
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  device = cpu
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  batch_size = 999
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  max_sen_len = None
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  num_labels = 15
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  vocab_size = 21128
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  hidden_size = 768
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  num_hidden_layers = 12
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  num_attention_heads = 12
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  hidden_act = gelu
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  intermediate_size = 3072
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  pad_token_id = 0
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  hidden_dropout_prob = 0.1
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  max_position_embeddings = 512
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  type_vocab_size = 2
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  initializer_range = 0.02
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  directionality = bidi
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  pooler_fc_size = 768
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  pooler_num_attention_heads = 12
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  pooler_num_fc_layers = 3
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  pooler_size_per_head = 128
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  pooler_type = first_token_transform
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  lstm_embedding_size = 768
2024-06-18 17:16:15 - INFO - [inference.py:243] - ###  lstm_dropout_prob = 0.5
2024-06-18 17:16:16 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 17:16:17 - INFO - [inference.py:252] - ## 成功载入已有模型，进行预测......
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:17 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:24 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:24 - INFO - [inference.py:316] - --------Dataset Build!--------
2024-06-18 17:16:24 - INFO - [inference.py:320] - --------Get Data-loader!--------
2024-06-18 17:16:24 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 17:16:24 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 17:16:24 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 17:16:26 - INFO - [inference.py:325] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 17:16:26 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:36 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 18:21:36 - INFO - [inference.py:212] -  ### 将当前配置打印到日志文件中 
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  device = cpu
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  batch_size = 999
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  max_sen_len = None
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  num_labels = 15
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  vocab_size = 21128
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  hidden_size = 768
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  num_hidden_layers = 12
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  num_attention_heads = 12
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  hidden_act = gelu
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  intermediate_size = 3072
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  pad_token_id = 0
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  hidden_dropout_prob = 0.1
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  max_position_embeddings = 512
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  type_vocab_size = 2
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  initializer_range = 0.02
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  directionality = bidi
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  pooler_fc_size = 768
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  pooler_num_attention_heads = 12
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  pooler_num_fc_layers = 3
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  pooler_size_per_head = 128
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  pooler_type = first_token_transform
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  lstm_embedding_size = 768
2024-06-18 18:21:36 - INFO - [inference.py:214] - ###  lstm_dropout_prob = 0.5
2024-06-18 18:21:37 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 18:21:38 - INFO - [inference.py:223] - ## 成功载入已有模型，进行预测......
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:38 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:46 - INFO - [inference.py:288] - --------Dataset Build!--------
2024-06-18 18:21:46 - INFO - [inference.py:292] - --------Get Data-loader!--------
2024-06-18 18:21:46 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 18:21:46 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 18:21:46 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 18:21:47 - INFO - [inference.py:297] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:21:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:23:59 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 18:23:59 - INFO - [inference.py:212] -  ### 将当前配置打印到日志文件中 
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  device = cpu
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  batch_size = 999
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  max_sen_len = None
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  num_labels = 15
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  vocab_size = 21128
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  hidden_size = 768
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  num_hidden_layers = 12
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  num_attention_heads = 12
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  hidden_act = gelu
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  intermediate_size = 3072
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  pad_token_id = 0
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  hidden_dropout_prob = 0.1
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  max_position_embeddings = 512
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  type_vocab_size = 2
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  initializer_range = 0.02
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  directionality = bidi
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  pooler_fc_size = 768
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  pooler_num_attention_heads = 12
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  pooler_num_fc_layers = 3
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  pooler_size_per_head = 128
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  pooler_type = first_token_transform
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  lstm_embedding_size = 768
2024-06-18 18:23:59 - INFO - [inference.py:214] - ###  lstm_dropout_prob = 0.5
2024-06-18 18:24:00 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 18:24:01 - INFO - [inference.py:223] - ## 成功载入已有模型，进行预测......
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:08 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:08 - INFO - [inference.py:288] - --------Dataset Build!--------
2024-06-18 18:24:08 - INFO - [inference.py:292] - --------Get Data-loader!--------
2024-06-18 18:24:08 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 18:24:08 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 18:24:08 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 18:24:10 - INFO - [inference.py:297] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:24:10 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:00 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 18:29:00 - INFO - [inference.py:212] -  ### 将当前配置打印到日志文件中 
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  device = cpu
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  batch_size = 999
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  max_sen_len = None
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  num_labels = 15
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  vocab_size = 21128
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  hidden_size = 768
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  num_hidden_layers = 12
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  num_attention_heads = 12
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  hidden_act = gelu
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  intermediate_size = 3072
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  pad_token_id = 0
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  hidden_dropout_prob = 0.1
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  max_position_embeddings = 512
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  type_vocab_size = 2
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  initializer_range = 0.02
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  directionality = bidi
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  pooler_fc_size = 768
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  pooler_num_attention_heads = 12
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  pooler_num_fc_layers = 3
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  pooler_size_per_head = 128
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  pooler_type = first_token_transform
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  lstm_embedding_size = 768
2024-06-18 18:29:00 - INFO - [inference.py:214] - ###  lstm_dropout_prob = 0.5
2024-06-18 18:29:01 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 18:29:01 - INFO - [inference.py:223] - ## 成功载入已有模型，进行预测......
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:13 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:13 - INFO - [inference.py:288] - --------Dataset Build!--------
2024-06-18 18:29:13 - INFO - [inference.py:292] - --------Get Data-loader!--------
2024-06-18 18:29:13 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 18:29:13 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 18:29:13 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 18:29:14 - INFO - [inference.py:297] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:29:14 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:36 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 18:40:36 - INFO - [inference.py:212] -  ### 将当前配置打印到日志文件中 
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  device = cpu
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  batch_size = 999
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  max_sen_len = None
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  num_labels = 15
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  vocab_size = 21128
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  hidden_size = 768
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  num_hidden_layers = 12
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  num_attention_heads = 12
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  hidden_act = gelu
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  intermediate_size = 3072
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  pad_token_id = 0
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  hidden_dropout_prob = 0.1
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  max_position_embeddings = 512
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  type_vocab_size = 2
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  initializer_range = 0.02
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  directionality = bidi
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  pooler_fc_size = 768
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  pooler_num_attention_heads = 12
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  pooler_num_fc_layers = 3
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  pooler_size_per_head = 128
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  pooler_type = first_token_transform
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  lstm_embedding_size = 768
2024-06-18 18:40:36 - INFO - [inference.py:214] - ###  lstm_dropout_prob = 0.5
2024-06-18 18:40:37 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 18:40:38 - INFO - [inference.py:223] - ## 成功载入已有模型，进行预测......
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:38 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:47 - INFO - [inference.py:288] - --------Dataset Build!--------
2024-06-18 18:40:47 - INFO - [inference.py:292] - --------Get Data-loader!--------
2024-06-18 18:40:47 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 18:40:47 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 18:40:47 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 18:40:49 - INFO - [inference.py:297] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:40:49 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:20 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 18:44:20 - INFO - [inference.py:212] -  ### 将当前配置打印到日志文件中 
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  device = cpu
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  batch_size = 999
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  max_sen_len = None
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  num_labels = 15
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  vocab_size = 21128
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  hidden_size = 768
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  num_hidden_layers = 12
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  num_attention_heads = 12
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  hidden_act = gelu
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  intermediate_size = 3072
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  pad_token_id = 0
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  hidden_dropout_prob = 0.1
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  max_position_embeddings = 512
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  type_vocab_size = 2
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  initializer_range = 0.02
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  directionality = bidi
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  pooler_fc_size = 768
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  pooler_num_attention_heads = 12
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  pooler_num_fc_layers = 3
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  pooler_size_per_head = 128
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  pooler_type = first_token_transform
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  lstm_embedding_size = 768
2024-06-18 18:44:20 - INFO - [inference.py:214] - ###  lstm_dropout_prob = 0.5
2024-06-18 18:44:21 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 18:44:21 - INFO - [inference.py:223] - ## 成功载入已有模型，进行预测......
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:31 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:31 - INFO - [inference.py:288] - --------Dataset Build!--------
2024-06-18 18:44:31 - INFO - [inference.py:292] - --------Get Data-loader!--------
2024-06-18 18:44:31 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 18:44:31 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 18:44:31 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 18:44:32 - INFO - [inference.py:297] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:44:32 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:45 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\config.json
2024-06-18 18:45:45 - INFO - [inference.py:212] -  ### 将当前配置打印到日志文件中 
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  project_dir = e:/pythonProject/pytorch/ExtractInfo
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  dataset_dir = e:/pythonProject/pytorch/ExtractInfo\data\SingleSentenceClassification
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  pretrained_model_dir = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  vocab_path = e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  device = cpu
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  model_save_dir = e:/pythonProject/pytorch/ExtractInfo\trained_models\model_for_sentences_classification
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  batch_size = 999
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  max_sen_len = None
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  num_labels = 15
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  vocab_size = 21128
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  hidden_size = 768
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  num_hidden_layers = 12
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  num_attention_heads = 12
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  hidden_act = gelu
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  intermediate_size = 3072
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  pad_token_id = 0
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  hidden_dropout_prob = 0.1
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  attention_probs_dropout_prob = 0.1
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  max_position_embeddings = 512
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  type_vocab_size = 2
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  initializer_range = 0.02
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  directionality = bidi
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  pooler_fc_size = 768
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  pooler_num_attention_heads = 12
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  pooler_num_fc_layers = 3
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  pooler_size_per_head = 128
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  pooler_type = first_token_transform
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  lstm_embedding_size = 768
2024-06-18 18:45:45 - INFO - [inference.py:214] - ###  lstm_dropout_prob = 0.5
2024-06-18 18:45:46 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 18:45:46 - INFO - [inference.py:223] - ## 成功载入已有模型，进行预测......
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:323] - Model name 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:359] - Didn't find file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:395] - loading file e:/pythonProject/pytorch/ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:46 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:56 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:56 - INFO - [inference.py:288] - --------Dataset Build!--------
2024-06-18 18:45:56 - INFO - [inference.py:292] - --------Get Data-loader!--------
2024-06-18 18:45:56 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 18:45:56 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 18:45:56 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 18:45:57 - INFO - [inference.py:297] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:45:57 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:06 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\config.json
2024-06-18 18:53:07 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 18:53:07 - INFO - [inference.py:49] - ## 成功载入已有模型，进行预测......
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:07 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:21 - INFO - [inference.py:114] - --------Dataset Build!--------
2024-06-18 18:53:21 - INFO - [inference.py:118] - --------Get Data-loader!--------
2024-06-18 18:53:21 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\config.json
2024-06-18 18:53:21 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 18:53:21 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 18:53:23 - INFO - [inference.py:123] - --------Load model from e:\pythonProject\pytorch\ExtractInfo\trained_models\model_for_NER--------
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\ExtractInfo\pretrained_Bert\vocab.txt
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 18:53:23 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:27:59 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-06-18 21:28:00 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-18 21:28:01 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:19 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:19 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-06-18 21:28:19 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-06-18 21:28:19 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-06-18 21:28:19 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-18 21:28:19 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-06-18 21:28:21 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-18 21:28:21 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:37 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-06-20 17:01:39 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-20 17:01:39 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:39 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:50 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:50 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-06-20 17:01:50 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-06-20 17:01:50 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-06-20 17:01:50 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-20 17:01:50 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-06-20 17:01:51 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 17:01:51 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:43:59 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-06-20 20:44:00 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-20 20:44:01 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:44:01 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:45:31 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:45:31 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-06-20 20:45:31 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-06-20 20:45:31 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-06-20 20:45:31 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-20 20:45:31 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-06-20 20:45:33 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:45:33 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:49:40 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-06-20 20:49:41 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-20 20:49:42 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:49:42 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:40 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-06-20 20:51:41 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-06-20 20:51:41 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:41 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:52 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:52 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-06-20 20:51:52 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-06-20 20:51:52 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-06-20 20:51:52 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-06-20 20:51:52 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-06-20 20:51:53 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:395] - loading file None
2024-06-20 20:51:53 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:39 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-08-12 16:03:40 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-08-12 16:03:41 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:41 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:51 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:51 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-08-12 16:03:51 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-08-12 16:03:51 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-08-12 16:03:51 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-08-12 16:03:51 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-08-12 16:03:52 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:03:52 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:30 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-08-12 16:20:32 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-08-12 16:20:32 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:32 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:44 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:44 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-08-12 16:20:44 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-08-12 16:20:44 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-08-12 16:20:44 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-08-12 16:20:44 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-08-12 16:20:45 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:20:45 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:33 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-08-12 16:24:34 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-08-12 16:24:34 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:34 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:45 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:45 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-08-12 16:24:45 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-08-12 16:24:45 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-08-12 16:24:45 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-08-12 16:24:45 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-08-12 16:24:47 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-12 16:24:47 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:10:59 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-08-13 11:11:00 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-08-13 11:11:00 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:00 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:09 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:09 - INFO - [inference.py:108] - --------Dataset Build!--------
2024-08-13 11:11:09 - INFO - [inference.py:112] - --------Get Data-loader!--------
2024-08-13 11:11:09 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-08-13 11:11:09 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-08-13 11:11:09 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-08-13 11:11:10 - INFO - [inference.py:117] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-13 11:11:10 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:06:24 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-08-14 15:06:25 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-08-14 15:10:33 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-08-14 15:10:34 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-08-14 15:14:26 - INFO - [BertConfig.py:73] - 成功导入BERT配置文件 e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\config.json
2024-08-14 15:14:27 - INFO - [Bert.py:351] - ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现
2024-08-14 15:14:28 - INFO - [inference.py:43] - ## 成功载入已有模型，进行预测......
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:28 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:33 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:33 - INFO - [inference.py:132] - --------Dataset Build!--------
2024-08-14 15:14:33 - INFO - [inference.py:136] - --------Get Data-loader!--------
2024-08-14 15:14:33 - INFO - [configuration_utils.py:157] - loading configuration file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\config.json
2024-08-14 15:14:33 - INFO - [configuration_utils.py:177] - Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 768,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 27,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2024-08-14 15:14:33 - INFO - [modeling_utils.py:398] - loading weights file e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER\pytorch_model.bin
2024-08-14 15:14:34 - INFO - [inference.py:141] - --------Load model from e:\pythonProject\pytorch\intelTextExtraction\trained_models\model_for_NER--------
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:323] - Model name 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert' is a path or url to a directory containing tokenizer files.
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\added_tokens.json. We won't load it.
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\special_tokens_map.json. We won't load it.
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:359] - Didn't find file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\tokenizer_config.json. We won't load it.
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:395] - loading file e:\pythonProject\pytorch\intelTextExtraction\pretrained_Bert\vocab.txt
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:395] - loading file None
2024-08-14 15:14:34 - INFO - [tokenization_utils.py:395] - loading file None
